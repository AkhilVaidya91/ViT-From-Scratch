{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ff5f39",
   "metadata": {},
   "source": [
    "## __Training ViT from scratch on CIFAR 100__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25cbf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
    "# CIFAR100_STD = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "# IMAGE_SIZE = 32\n",
    "# NUM_CHANNELS = 3\n",
    "# NUM_CLASSES = 100\n",
    "\n",
    "# PATCH_SIZE = 4\n",
    "# HIDDEN_SIZE = 384\n",
    "# TRANSFORMER_BLOCKS = 10\n",
    "# ATTENTION_HEADS = 8\n",
    "# MLP_DIMENSION = 4 * HIDDEN_SIZE\n",
    "# NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# WARMUP_PCT = 0.1\n",
    "# EPOCHS = 100\n",
    "# BATCH_SIZE = 512\n",
    "# LEARNING_RATE = 6e-4\n",
    "# WEIGHT_DECAY = 0.05\n",
    "# DROPOUT = 0.15\n",
    "# LABEL_SMOOTHING = 0.1\n",
    "\n",
    "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
    "CIFAR100_STD = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 100\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "HIDDEN_SIZE = 480\n",
    "TRANSFORMER_BLOCKS = 12\n",
    "ATTENTION_HEADS = 10\n",
    "MLP_DIMENSION = 4 * HIDDEN_SIZE\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "WARMUP_PCT = 0.1\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 7e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.2\n",
    "LABEL_SMOOTHING = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba27f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformation = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),  # Adds rotation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "])\n",
    "\n",
    "# Keep test transformation simple (only normalization)\n",
    "test_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=train_transformation)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50420f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.4487329..0.9871495].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([256, 3, 32, 32])\n",
      "Batch means: tensor([-0.3324, -0.3181, -0.2714]), Batch stds: tensor([1.1394, 1.1272, 1.0596])\n",
      "Total number of samples in train dataset: 50000\n",
      "Total number of samples in test dataset: 10000\n",
      "Total number of classes: 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## checking the dataloader\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter._next_data()\n",
    "print(f'Sample batch shape: {images.shape}')\n",
    "print(f'Batch means: {images.mean(dim=[0,2,3])}, Batch stds: {images.std(dim=[0,2,3])}')\n",
    "print(f'Total number of samples in train dataset: {len(train_dataset)}')\n",
    "print(f'Total number of samples in test dataset: {len(test_dataset)}')\n",
    "print(f'Total number of classes: {len(train_dataset.classes)}')\n",
    "\n",
    "# sample image\n",
    "img = images[10]\n",
    "img = img / 2 + 0.5\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2edfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding output shape: torch.Size([256, 64, 480])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(NUM_CHANNELS, HIDDEN_SIZE, kernel_size=PATCH_SIZE, stride=PATCH_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # (B, HIDDEN_SIZE, N_patches_sqrt, N_patches_sqrt)\n",
    "        x = x.flatten(2)         # (B, HIDDEN_SIZE, N_patches)\n",
    "        x = x.transpose(1, 2)    # (B, N_patches, HIDDEN_SIZE)\n",
    "        return x\n",
    "    \n",
    "example_patch_embedding = PatchEmbedding()\n",
    "data_batch = iter(train_loader)._next_data()[0]\n",
    "\n",
    "patches = example_patch_embedding(data_batch)\n",
    "print(f'Patch embedding output shape: {patches.shape}')  # (BATCH_SIZE, N_patches, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd630f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Encoder output shape: torch.Size([256, 64, 480])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.layer_norm2 = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.multihead_attention = nn.MultiheadAttention(\n",
    "            embed_dim=HIDDEN_SIZE, \n",
    "            num_heads=ATTENTION_HEADS, \n",
    "            batch_first=True,\n",
    "            dropout=dropout  # Add attention dropout\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, MLP_DIMENSION),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),  # Add dropout after GELU\n",
    "            nn.Linear(MLP_DIMENSION, HIDDEN_SIZE),\n",
    "            nn.Dropout(dropout)   # Add dropout after projection\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.layer_norm1(x)\n",
    "        attention_output, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "        x = x + attention_output\n",
    "        x_norm = self.layer_norm2(x)\n",
    "        mlp_output = self.mlp(x_norm)\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "    \n",
    "\n",
    "example_transformer_encoder = TransformerEncoder()\n",
    "encoder_output = example_transformer_encoder(patches)\n",
    "print(f'Transformer Encoder output shape: {encoder_output.shape}')  # (BATCH_SIZE, N_patches, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dab80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Head output shape: torch.Size([256, 100])\n"
     ]
    }
   ],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "example_mlp_head = MLPHead()\n",
    "mlp_output = example_mlp_head(encoder_output[:, 0, :])  # (BATCH_SIZE, NUM_CLASSES)\n",
    "print(f'MLP Head output shape: {mlp_output.shape}')  # (BATCH_SIZE, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc62ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, HIDDEN_SIZE))\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, NUM_PATCHES + 1, HIDDEN_SIZE))\n",
    "        self.dropout = nn.Dropout(dropout)  # Add embedding dropout\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerEncoder(dropout) for _ in range(TRANSFORMER_BLOCKS)])\n",
    "        self.mlp_head = MLPHead()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fe47547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VisionTransformer().to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = max(1, int(WARMUP_PCT * total_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbaf33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 0.0172, Accuracy: 0.0446\n",
      "Total correct predictions on test set: 793 out of 10000\n",
      "Test Accuracy: 0.0793\n",
      "Epoch [2/80], Loss: 0.0154, Accuracy: 0.1227\n",
      "Total correct predictions on test set: 1707 out of 10000\n",
      "Test Accuracy: 0.1707\n",
      "Epoch [3/80], Loss: 0.0141, Accuracy: 0.1918\n",
      "Total correct predictions on test set: 2356 out of 10000\n",
      "Test Accuracy: 0.2356\n",
      "Epoch [4/80], Loss: 0.0133, Accuracy: 0.2340\n",
      "Total correct predictions on test set: 2633 out of 10000\n",
      "Test Accuracy: 0.2633\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping (global norm = 1)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        total_correct += correct\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    accuracy = total_correct / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "        accuracy = total_correct / len(test_loader.dataset)\n",
    "        print(f'Total correct predictions on test set: {total_correct} out of {len(test_loader.dataset)}')\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18575c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vit_cifar100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = VisionTransformer().to(device)\n",
    "saved_model.load_state_dict(torch.load('vit_cifar100.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
