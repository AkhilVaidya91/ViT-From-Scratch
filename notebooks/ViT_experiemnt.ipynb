{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ff5f39",
   "metadata": {},
   "source": [
    "## __Training ViT from scratch on GTSRB Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f69c66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import GTSRB\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25cbf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard GTSRB normalization constants\n",
    "MEAN = (0.3337, 0.3064, 0.3171)\n",
    "STD = (0.2672, 0.2564, 0.2629)\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "HIDDEN_SIZE = 384\n",
    "TRANSFORMER_BLOCKS = 10\n",
    "ATTENTION_HEADS = 8\n",
    "MLP_DIMENSION = 4 * HIDDEN_SIZE\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "WARMUP_PCT = 0.1\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 6e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "DROPOUT = 0.15\n",
    "LABEL_SMOOTHING = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cba27f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187M/187M [00:16<00:00, 11.2MB/s] \n",
      "100%|██████████| 89.0M/89.0M [00:10<00:00, 8.45MB/s]\n",
      "100%|██████████| 99.6k/99.6k [00:00<00:00, 120kB/s] \n"
     ]
    }
   ],
   "source": [
    "MEAN = (0.3337, 0.3064, 0.3171)\n",
    "STD = (0.2672, 0.2564, 0.2629)\n",
    "\n",
    "def apply_clahe(img):\n",
    "    img_np = np.array(img)\n",
    "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    final_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "    return final_img\n",
    "\n",
    "train_transformation = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Lambda(lambda x: apply_clahe(x)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "test_transformation = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.Lambda(lambda x: apply_clahe(x)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "])\n",
    "\n",
    "train_dataset = GTSRB(root='./data', split='train', download=True, transform=train_transformation)\n",
    "test_dataset = GTSRB(root='./data', split='test', download=True, transform=test_transformation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50420f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.12443864..1.8525741].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch shape: torch.Size([512, 3, 32, 32])\n",
      "Batch means: tensor([0.2169, 0.2469, 0.2560]), Batch stds: tensor([1.0683, 1.0767, 1.0900])\n",
      "Total number of samples in train dataset: 26640\n",
      "Total number of samples in test dataset: 12630\n",
      "Total number of classes: 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## checking the dataloader\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter._next_data()\n",
    "print(f'Sample batch shape: {images.shape}')\n",
    "print(f'Batch means: {images.mean(dim=[0,2,3])}, Batch stds: {images.std(dim=[0,2,3])}')\n",
    "print(f'Total number of samples in train dataset: {len(train_dataset)}')\n",
    "print(f'Total number of samples in test dataset: {len(test_dataset)}')\n",
    "print(f'Total number of classes: 43')\n",
    "\n",
    "# sample image\n",
    "img = images[10]\n",
    "img = img / 2 + 0.5\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2edfdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embedding output shape: torch.Size([512, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv2d(NUM_CHANNELS, HIDDEN_SIZE, kernel_size=PATCH_SIZE, stride=PATCH_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)  # (B, HIDDEN_SIZE, N_patches_sqrt, N_patches_sqrt)\n",
    "        x = x.flatten(2)         # (B, HIDDEN_SIZE, N_patches)\n",
    "        x = x.transpose(1, 2)    # (B, N_patches, HIDDEN_SIZE)\n",
    "        return x\n",
    "    \n",
    "example_patch_embedding = PatchEmbedding()\n",
    "data_batch = iter(train_loader)._next_data()[0]\n",
    "\n",
    "patches = example_patch_embedding(data_batch)\n",
    "print(f'Patch embedding output shape: {patches.shape}')  # (BATCH_SIZE, N_patches, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd630f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Encoder output shape: torch.Size([512, 64, 384])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.layer_norm2 = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.multihead_attention = nn.MultiheadAttention(\n",
    "            embed_dim=HIDDEN_SIZE, \n",
    "            num_heads=ATTENTION_HEADS, \n",
    "            batch_first=True,\n",
    "            dropout=dropout  # Add attention dropout\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE, MLP_DIMENSION),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),  # Add dropout after GELU\n",
    "            nn.Linear(MLP_DIMENSION, HIDDEN_SIZE),\n",
    "            nn.Dropout(dropout)   # Add dropout after projection\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.layer_norm1(x)\n",
    "        attention_output, _ = self.multihead_attention(x_norm, x_norm, x_norm)\n",
    "        x = x + attention_output\n",
    "        x_norm = self.layer_norm2(x)\n",
    "        mlp_output = self.mlp(x_norm)\n",
    "        x = x + mlp_output\n",
    "        return x\n",
    "    \n",
    "\n",
    "example_transformer_encoder = TransformerEncoder()\n",
    "encoder_output = example_transformer_encoder(patches)\n",
    "print(f'Transformer Encoder output shape: {encoder_output.shape}')  # (BATCH_SIZE, N_patches, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dab80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Head output shape: torch.Size([512, 43])\n"
     ]
    }
   ],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "example_mlp_head = MLPHead()\n",
    "mlp_output = example_mlp_head(encoder_output[:, 0, :])  # (BATCH_SIZE, NUM_CLASSES)\n",
    "print(f'MLP Head output shape: {mlp_output.shape}')  # (BATCH_SIZE, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cc62ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, HIDDEN_SIZE))\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, NUM_PATCHES + 1, HIDDEN_SIZE))\n",
    "        self.dropout = nn.Dropout(dropout)  # Add embedding dropout\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerEncoder(dropout) for _ in range(TRANSFORMER_BLOCKS)])\n",
    "        self.mlp_head = MLPHead()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fe47547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VisionTransformer().to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = max(1, int(WARMUP_PCT * total_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbaf33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b1f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 0.0071, Accuracy: 0.0801\n",
      "Total correct predictions on test set: 1479 out of 12630\n",
      "Test Accuracy: 0.1171\n",
      "Epoch [2/80], Loss: 0.0064, Accuracy: 0.1487\n",
      "Total correct predictions on test set: 2123 out of 12630\n",
      "Test Accuracy: 0.1681\n",
      "Epoch [3/80], Loss: 0.0051, Accuracy: 0.3209\n",
      "Total correct predictions on test set: 6219 out of 12630\n",
      "Test Accuracy: 0.4924\n",
      "Epoch [4/80], Loss: 0.0039, Accuracy: 0.5215\n",
      "Total correct predictions on test set: 6993 out of 12630\n",
      "Test Accuracy: 0.5537\n",
      "Epoch [5/80], Loss: 0.0032, Accuracy: 0.6693\n",
      "Total correct predictions on test set: 9039 out of 12630\n",
      "Test Accuracy: 0.7157\n",
      "Epoch [6/80], Loss: 0.0026, Accuracy: 0.7855\n",
      "Total correct predictions on test set: 10165 out of 12630\n",
      "Test Accuracy: 0.8048\n",
      "Epoch [7/80], Loss: 0.0022, Accuracy: 0.8553\n",
      "Total correct predictions on test set: 10940 out of 12630\n",
      "Test Accuracy: 0.8662\n",
      "Epoch [8/80], Loss: 0.0021, Accuracy: 0.8895\n",
      "Total correct predictions on test set: 11333 out of 12630\n",
      "Test Accuracy: 0.8973\n",
      "Epoch [9/80], Loss: 0.0019, Accuracy: 0.9189\n",
      "Total correct predictions on test set: 11397 out of 12630\n",
      "Test Accuracy: 0.9024\n",
      "Epoch [10/80], Loss: 0.0018, Accuracy: 0.9384\n",
      "Total correct predictions on test set: 11195 out of 12630\n",
      "Test Accuracy: 0.8864\n",
      "Epoch [11/80], Loss: 0.0017, Accuracy: 0.9497\n",
      "Total correct predictions on test set: 11676 out of 12630\n",
      "Test Accuracy: 0.9245\n",
      "Epoch [12/80], Loss: 0.0017, Accuracy: 0.9599\n",
      "Total correct predictions on test set: 11804 out of 12630\n",
      "Test Accuracy: 0.9346\n",
      "Epoch [13/80], Loss: 0.0016, Accuracy: 0.9659\n",
      "Total correct predictions on test set: 11638 out of 12630\n",
      "Test Accuracy: 0.9215\n",
      "Epoch [14/80], Loss: 0.0016, Accuracy: 0.9705\n",
      "Total correct predictions on test set: 11901 out of 12630\n",
      "Test Accuracy: 0.9423\n",
      "Epoch [15/80], Loss: 0.0016, Accuracy: 0.9735\n",
      "Total correct predictions on test set: 11955 out of 12630\n",
      "Test Accuracy: 0.9466\n",
      "Epoch [16/80], Loss: 0.0015, Accuracy: 0.9739\n",
      "Total correct predictions on test set: 11945 out of 12630\n",
      "Test Accuracy: 0.9458\n",
      "Epoch [17/80], Loss: 0.0015, Accuracy: 0.9770\n",
      "Total correct predictions on test set: 11925 out of 12630\n",
      "Test Accuracy: 0.9442\n",
      "Epoch [18/80], Loss: 0.0015, Accuracy: 0.9812\n",
      "Total correct predictions on test set: 11883 out of 12630\n",
      "Test Accuracy: 0.9409\n",
      "Epoch [19/80], Loss: 0.0015, Accuracy: 0.9804\n",
      "Total correct predictions on test set: 11967 out of 12630\n",
      "Test Accuracy: 0.9475\n",
      "Epoch [20/80], Loss: 0.0015, Accuracy: 0.9799\n",
      "Total correct predictions on test set: 11933 out of 12630\n",
      "Test Accuracy: 0.9448\n",
      "Epoch [21/80], Loss: 0.0015, Accuracy: 0.9818\n",
      "Total correct predictions on test set: 11986 out of 12630\n",
      "Test Accuracy: 0.9490\n",
      "Epoch [22/80], Loss: 0.0015, Accuracy: 0.9834\n",
      "Total correct predictions on test set: 11994 out of 12630\n",
      "Test Accuracy: 0.9496\n",
      "Epoch [23/80], Loss: 0.0015, Accuracy: 0.9850\n",
      "Total correct predictions on test set: 12039 out of 12630\n",
      "Test Accuracy: 0.9532\n",
      "Epoch [24/80], Loss: 0.0015, Accuracy: 0.9849\n",
      "Total correct predictions on test set: 12081 out of 12630\n",
      "Test Accuracy: 0.9565\n",
      "Epoch [25/80], Loss: 0.0015, Accuracy: 0.9856\n",
      "Total correct predictions on test set: 11913 out of 12630\n",
      "Test Accuracy: 0.9432\n",
      "Epoch [26/80], Loss: 0.0014, Accuracy: 0.9873\n",
      "Total correct predictions on test set: 12013 out of 12630\n",
      "Test Accuracy: 0.9511\n",
      "Epoch [27/80], Loss: 0.0014, Accuracy: 0.9883\n",
      "Total correct predictions on test set: 11974 out of 12630\n",
      "Test Accuracy: 0.9481\n",
      "Epoch [28/80], Loss: 0.0014, Accuracy: 0.9889\n",
      "Total correct predictions on test set: 11954 out of 12630\n",
      "Test Accuracy: 0.9465\n",
      "Epoch [29/80], Loss: 0.0014, Accuracy: 0.9890\n",
      "Total correct predictions on test set: 11982 out of 12630\n",
      "Test Accuracy: 0.9487\n",
      "Epoch [30/80], Loss: 0.0014, Accuracy: 0.9910\n",
      "Total correct predictions on test set: 11995 out of 12630\n",
      "Test Accuracy: 0.9497\n",
      "Epoch [31/80], Loss: 0.0014, Accuracy: 0.9899\n",
      "Total correct predictions on test set: 12058 out of 12630\n",
      "Test Accuracy: 0.9547\n",
      "Epoch [32/80], Loss: 0.0014, Accuracy: 0.9903\n",
      "Total correct predictions on test set: 12012 out of 12630\n",
      "Test Accuracy: 0.9511\n",
      "Epoch [33/80], Loss: 0.0014, Accuracy: 0.9898\n",
      "Total correct predictions on test set: 12133 out of 12630\n",
      "Test Accuracy: 0.9606\n",
      "Epoch [34/80], Loss: 0.0014, Accuracy: 0.9923\n",
      "Total correct predictions on test set: 11970 out of 12630\n",
      "Test Accuracy: 0.9477\n",
      "Epoch [35/80], Loss: 0.0014, Accuracy: 0.9899\n",
      "Total correct predictions on test set: 11975 out of 12630\n",
      "Test Accuracy: 0.9481\n",
      "Epoch [36/80], Loss: 0.0014, Accuracy: 0.9925\n",
      "Total correct predictions on test set: 11955 out of 12630\n",
      "Test Accuracy: 0.9466\n",
      "Epoch [37/80], Loss: 0.0014, Accuracy: 0.9901\n",
      "Total correct predictions on test set: 12041 out of 12630\n",
      "Test Accuracy: 0.9534\n",
      "Epoch [38/80], Loss: 0.0014, Accuracy: 0.9908\n",
      "Total correct predictions on test set: 12076 out of 12630\n",
      "Test Accuracy: 0.9561\n",
      "Epoch [39/80], Loss: 0.0014, Accuracy: 0.9924\n",
      "Total correct predictions on test set: 12067 out of 12630\n",
      "Test Accuracy: 0.9554\n",
      "Epoch [40/80], Loss: 0.0014, Accuracy: 0.9934\n",
      "Total correct predictions on test set: 12090 out of 12630\n",
      "Test Accuracy: 0.9572\n",
      "Epoch [41/80], Loss: 0.0014, Accuracy: 0.9926\n",
      "Total correct predictions on test set: 12141 out of 12630\n",
      "Test Accuracy: 0.9613\n",
      "Epoch [42/80], Loss: 0.0014, Accuracy: 0.9926\n",
      "Total correct predictions on test set: 12136 out of 12630\n",
      "Test Accuracy: 0.9609\n",
      "Epoch [43/80], Loss: 0.0014, Accuracy: 0.9943\n",
      "Total correct predictions on test set: 12165 out of 12630\n",
      "Test Accuracy: 0.9632\n",
      "Epoch [44/80], Loss: 0.0014, Accuracy: 0.9951\n",
      "Total correct predictions on test set: 12101 out of 12630\n",
      "Test Accuracy: 0.9581\n",
      "Epoch [45/80], Loss: 0.0014, Accuracy: 0.9959\n",
      "Total correct predictions on test set: 12111 out of 12630\n",
      "Test Accuracy: 0.9589\n",
      "Epoch [46/80], Loss: 0.0014, Accuracy: 0.9952\n",
      "Total correct predictions on test set: 12121 out of 12630\n",
      "Test Accuracy: 0.9597\n",
      "Epoch [47/80], Loss: 0.0014, Accuracy: 0.9969\n",
      "Total correct predictions on test set: 12168 out of 12630\n",
      "Test Accuracy: 0.9634\n",
      "Epoch [48/80], Loss: 0.0014, Accuracy: 0.9966\n",
      "Total correct predictions on test set: 12155 out of 12630\n",
      "Test Accuracy: 0.9624\n",
      "Epoch [49/80], Loss: 0.0014, Accuracy: 0.9969\n",
      "Total correct predictions on test set: 12090 out of 12630\n",
      "Test Accuracy: 0.9572\n",
      "Epoch [50/80], Loss: 0.0014, Accuracy: 0.9966\n",
      "Total correct predictions on test set: 12174 out of 12630\n",
      "Test Accuracy: 0.9639\n",
      "Epoch [51/80], Loss: 0.0014, Accuracy: 0.9975\n",
      "Total correct predictions on test set: 12165 out of 12630\n",
      "Test Accuracy: 0.9632\n",
      "Epoch [52/80], Loss: 0.0014, Accuracy: 0.9974\n",
      "Total correct predictions on test set: 12131 out of 12630\n",
      "Test Accuracy: 0.9605\n",
      "Epoch [53/80], Loss: 0.0014, Accuracy: 0.9978\n",
      "Total correct predictions on test set: 12079 out of 12630\n",
      "Test Accuracy: 0.9564\n",
      "Epoch [54/80], Loss: 0.0014, Accuracy: 0.9983\n",
      "Total correct predictions on test set: 12131 out of 12630\n",
      "Test Accuracy: 0.9605\n",
      "Epoch [55/80], Loss: 0.0014, Accuracy: 0.9985\n",
      "Total correct predictions on test set: 12135 out of 12630\n",
      "Test Accuracy: 0.9608\n",
      "Epoch [56/80], Loss: 0.0014, Accuracy: 0.9982\n",
      "Total correct predictions on test set: 12197 out of 12630\n",
      "Test Accuracy: 0.9657\n",
      "Epoch [57/80], Loss: 0.0014, Accuracy: 0.9984\n",
      "Total correct predictions on test set: 12128 out of 12630\n",
      "Test Accuracy: 0.9603\n",
      "Epoch [58/80], Loss: 0.0014, Accuracy: 0.9984\n",
      "Total correct predictions on test set: 12180 out of 12630\n",
      "Test Accuracy: 0.9644\n",
      "Epoch [59/80], Loss: 0.0014, Accuracy: 0.9987\n",
      "Total correct predictions on test set: 12156 out of 12630\n",
      "Test Accuracy: 0.9625\n",
      "Epoch [60/80], Loss: 0.0014, Accuracy: 0.9988\n",
      "Total correct predictions on test set: 12132 out of 12630\n",
      "Test Accuracy: 0.9606\n",
      "Epoch [61/80], Loss: 0.0014, Accuracy: 0.9988\n",
      "Total correct predictions on test set: 12161 out of 12630\n",
      "Test Accuracy: 0.9629\n",
      "Epoch [62/80], Loss: 0.0014, Accuracy: 0.9989\n",
      "Total correct predictions on test set: 12129 out of 12630\n",
      "Test Accuracy: 0.9603\n",
      "Epoch [63/80], Loss: 0.0014, Accuracy: 0.9990\n",
      "Total correct predictions on test set: 12158 out of 12630\n",
      "Test Accuracy: 0.9626\n",
      "Epoch [64/80], Loss: 0.0014, Accuracy: 0.9991\n",
      "Total correct predictions on test set: 12161 out of 12630\n",
      "Test Accuracy: 0.9629\n",
      "Epoch [65/80], Loss: 0.0014, Accuracy: 0.9987\n",
      "Total correct predictions on test set: 12159 out of 12630\n",
      "Test Accuracy: 0.9627\n",
      "Epoch [66/80], Loss: 0.0014, Accuracy: 0.9992\n",
      "Total correct predictions on test set: 12210 out of 12630\n",
      "Test Accuracy: 0.9667\n",
      "Epoch [67/80], Loss: 0.0014, Accuracy: 0.9993\n",
      "Total correct predictions on test set: 12155 out of 12630\n",
      "Test Accuracy: 0.9624\n",
      "Epoch [68/80], Loss: 0.0014, Accuracy: 0.9993\n",
      "Total correct predictions on test set: 12169 out of 12630\n",
      "Test Accuracy: 0.9635\n",
      "Epoch [69/80], Loss: 0.0014, Accuracy: 0.9995\n",
      "Total correct predictions on test set: 12146 out of 12630\n",
      "Test Accuracy: 0.9617\n",
      "Epoch [70/80], Loss: 0.0014, Accuracy: 0.9993\n",
      "Total correct predictions on test set: 12165 out of 12630\n",
      "Test Accuracy: 0.9632\n",
      "Epoch [71/80], Loss: 0.0014, Accuracy: 0.9995\n",
      "Total correct predictions on test set: 12195 out of 12630\n",
      "Test Accuracy: 0.9656\n",
      "Epoch [72/80], Loss: 0.0014, Accuracy: 0.9996\n",
      "Total correct predictions on test set: 12195 out of 12630\n",
      "Test Accuracy: 0.9656\n",
      "Epoch [73/80], Loss: 0.0014, Accuracy: 0.9995\n",
      "Total correct predictions on test set: 12188 out of 12630\n",
      "Test Accuracy: 0.9650\n",
      "Epoch [74/80], Loss: 0.0014, Accuracy: 0.9997\n",
      "Total correct predictions on test set: 12202 out of 12630\n",
      "Test Accuracy: 0.9661\n",
      "Epoch [75/80], Loss: 0.0014, Accuracy: 0.9997\n",
      "Total correct predictions on test set: 12200 out of 12630\n",
      "Test Accuracy: 0.9660\n",
      "Epoch [76/80], Loss: 0.0014, Accuracy: 0.9995\n",
      "Total correct predictions on test set: 12201 out of 12630\n",
      "Test Accuracy: 0.9660\n",
      "Epoch [77/80], Loss: 0.0014, Accuracy: 0.9992\n",
      "Total correct predictions on test set: 12203 out of 12630\n",
      "Test Accuracy: 0.9662\n",
      "Epoch [78/80], Loss: 0.0014, Accuracy: 0.9994\n",
      "Total correct predictions on test set: 12200 out of 12630\n",
      "Test Accuracy: 0.9660\n",
      "Epoch [79/80], Loss: 0.0014, Accuracy: 0.9995\n",
      "Total correct predictions on test set: 12198 out of 12630\n",
      "Test Accuracy: 0.9658\n",
      "Epoch [80/80], Loss: 0.0014, Accuracy: 0.9997\n",
      "Total correct predictions on test set: 12198 out of 12630\n",
      "Test Accuracy: 0.9658\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        total_correct += correct\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    accuracy = total_correct / len(train_loader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "        accuracy = total_correct / len(test_loader.dataset)\n",
    "        print(f'Total correct predictions on test set: {total_correct} out of {len(test_loader.dataset)}')\n",
    "        print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18575c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'vit_gtsrb_clahe.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0644ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = VisionTransformer().to(device)\n",
    "saved_model.load_state_dict(torch.load('vit_gtsrb_clahe.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
